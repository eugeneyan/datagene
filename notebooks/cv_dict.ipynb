{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import string\n",
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "import math\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "from HTMLParser import HTMLParser\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from eutils.utils.logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html_parser = HTMLParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/output/title_category_keep_samp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-12 12:14:30,877 - Data loaded of size: (991078, 3)\n",
      "INFO:__log__:Data loaded of size: (991078, 3)\n"
     ]
    }
   ],
   "source": [
    "# Exclude categories where count < 3\n",
    "cat_count = df[['title', 'category_path']].groupby('category_path').count().reset_index().sort_values(by='title')\n",
    "\n",
    "# Filter those with > 10 categories\n",
    "cat_count = cat_count[cat_count['title'] > 5]\n",
    "cat_count.rename(columns={'title': 'cat_count'}, inplace=True)\n",
    "\n",
    "# Filter df to only include those with cat_count > 5\n",
    "df = df.merge(cat_count, on='category_path', how='inner')\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['asin', 'title', 'category_path']]\n",
    "\n",
    "logger.info('Data loaded of size: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "SPAM_WORDS = {'import', 'export', 'day', 'week', 'month', 'year', 'new', 'free', 'international', 'intl', 'oem', ''}\n",
    "COLOURS = set(matplotlib.colors.cnames.keys())\n",
    "STOP_WORDS = STOP_WORDS.union(SPAM_WORDS).union(COLOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove records with no category form df\n",
    "def remove_no_category(df, category='category_path'):\n",
    "    \"\"\" (DataFrame, str) -> DataFrame\n",
    "\n",
    "    Returns a dataframe where the missing categories have been dropped.\n",
    "\n",
    "    :param df:\n",
    "    :param category:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[df[category] != np.nan]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to encode string\n",
    "def encode_string(title, parser):\n",
    "    \"\"\" (str) -> str\n",
    "\n",
    "    Returns a string that is encoded as ascii\n",
    "    Note: While unicode(title, 'utf-8', 'ignore') seems to work correctly in doctest, it has led to errors in the past.\n",
    "    If so, use iso-8859-1.\n",
    "\n",
    "    :param title:\n",
    "    :return:\n",
    "\n",
    "    >>> encode_string('Crème brûlée')\n",
    "    'Creme brulee'\n",
    "    >>> encode_string('åöûëî')\n",
    "    'aouei'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        encoded_title = unicodedata.normalize('NFKD', unicode(title, 'utf-8', 'ignore')).encode('ascii', 'ignore')\n",
    "        encoded_title = parser.unescape(encoded_title).encode('ascii', 'ignore')\n",
    "    except TypeError:  # if title is missing and a float\n",
    "        encoded_title = 'NA'\n",
    "\n",
    "    return encoded_title\n",
    "\n",
    "\n",
    "# Encode titles in df\n",
    "def encode_title(df, title='title_processed', parser=html_parser):\n",
    "    \"\"\" (DataFrame, str) -> DataFrame\n",
    "\n",
    "    Returns a dataframe where the title has been encoded.\n",
    "\n",
    "    :param df:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df[title] = df[title].apply(encode_string, args=(parser, ))\n",
    "    logger.info('{} encoded'.format(title))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Lowercase titles in df\n",
    "def lowercase_title(df, title='title_processed'):\n",
    "    \"\"\" (DataFrame, str) -> DataFrame\n",
    "\n",
    "    Returns a dataframe where the title has been lowercased.\n",
    "\n",
    "    :param df:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df[title] = df[title].apply(string.lower)\n",
    "    logger.info('{} lowercased'.format(title))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Tokenize strings\n",
    "def tokenize_title_string(title, excluded):\n",
    "    \"\"\" (str) -> list(str)\n",
    "\n",
    "    Returns a list of string tokens given a string.\n",
    "    It will exclude the following characters from the tokenization: - / . %\n",
    "\n",
    "    :param title:\n",
    "    :return:\n",
    "\n",
    "    >>> tokenize_title_string('hello world')\n",
    "    ['hello', 'world']\n",
    "    >>> tokenize_title_string('test hyphen-word 0.9 20% green/blue')\n",
    "    ['test', 'hyphen-word', '0.9', '20%', 'green/blue']\n",
    "    \"\"\"\n",
    "\n",
    "    return re.split(\"[^\" + excluded + \"\\w]+\", title)\n",
    "\n",
    "\n",
    "# Tokenize titles in df\n",
    "def tokenize_title(df, title='title', excluded='-/.%'):\n",
    "    \"\"\" (DataFrame, str) -> DataFrame\n",
    "\n",
    "    Returns a dataframe where the title has been tokenized based on function tokenize_title_string\n",
    "\n",
    "    :param df:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df[title] = df[title].apply(tokenize_title_string, args=(excluded, ))\n",
    "    logger.info('{} tokenized'.format(title))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Remove stopwords from string\n",
    "def remove_words(title, words_to_remove):\n",
    "    \"\"\" (list(str), set) -> list(str)\n",
    "\n",
    "    Returns a list of tokens where the stopwords/spam words/colours have been removed\n",
    "\n",
    "    :param title:\n",
    "    :param words_to_remove:\n",
    "    :return:\n",
    "    >>> remove_words(['python', 'is', 'the', 'best'], STOP_WORDS)\n",
    "    ['python', 'best']\n",
    "    >>> remove_words(['grapes', 'come', 'in', 'purple', 'and', 'green'], STOP_WORDS)\n",
    "    ['grapes', 'come']\n",
    "    >>> remove_words(['spammy', 'title', 'intl', 'buyincoins', 'export'], STOP_WORDS)\n",
    "    ['spammy', 'title']\n",
    "    \"\"\"\n",
    "\n",
    "    return [token for token in title if token not in words_to_remove]\n",
    "\n",
    "\n",
    "# Remove stopwords from df\n",
    "def remove_stopwords(df, stopwords, title='title_processed'):\n",
    "    \"\"\" (DataFrame, set, str) -> DataFrame\n",
    "\n",
    "    Returns a DataFrame where the stopwords have been removed from the titles\n",
    "\n",
    "    :param df:\n",
    "    :param stopwords:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df[title] = df[title].apply(remove_words, args=(stopwords, ))\n",
    "    logger.info('{} stopwords removed'.format(title))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Remove words with character count below threshold from string\n",
    "def remove_chars(title, word_len=1):\n",
    "    \"\"\" (list(str), int) -> list(str)\n",
    "\n",
    "    Returns a list of str (tokenized titles) where tokens of character length =< word_len is removed.\n",
    "\n",
    "    :param title:\n",
    "    :param word_len:\n",
    "    :return:\n",
    "\n",
    "    >>> remove_chars(['what', 'remains', 'of', 'a', 'word', '!', ''], 1)\n",
    "    ['what', 'remains', 'of', 'word']\n",
    "    >>> remove_chars(['what', 'remains', 'of', 'a', 'word', '!', '', 'if', 'word_len', 'is', '2'], 2)\n",
    "    ['what', 'remains', 'word', 'word_len']\n",
    "    \"\"\"\n",
    "\n",
    "    return [token for token in title if len(token) > word_len]\n",
    "\n",
    "\n",
    "# Remove words that are fully numeric\n",
    "def remove_numeric(title):\n",
    "    \"\"\" (list(str)) -> list(str)\n",
    "\n",
    "    Remove words which are fully numeric\n",
    "\n",
    "    :param title:\n",
    "    :return:\n",
    "\n",
    "    >>> remove_numeric(['A', 'B', '1', '123', 'C'])\n",
    "    ['A', 'B', 'C']\n",
    "    \"\"\"\n",
    "\n",
    "    return [token for token in title if not token.isdigit()]\n",
    "\n",
    "\n",
    "# Remove words that are solely numeric from df\n",
    "def remove_numeric_from_df(df, title='title_processed'):\n",
    "    df[title] = df[title].apply(remove_numeric)\n",
    "    logger.info('{} solely numeric words removed'.format(title))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Remove words that have words == 1 char from title\n",
    "def remove_one_char_words(df, word_len=1, title='title_processed'):\n",
    "    \"\"\" (DataFrame, int, str) -> DataFrame\n",
    "\n",
    "    Returns a DataFrame where tokens of character length <= word_len is removed\n",
    "\n",
    "    :param df:\n",
    "    :param word_len:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df[title] = df[title].apply(remove_chars, args=(word_len, ))\n",
    "    logger.info('{} tokens with char length equals {} removed'.format(title, word_len))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_ngrams(input_list, n):\n",
    "    \"\"\" list, int -> list(tuples)\n",
    "\n",
    "    Return a list of ngram tuples, where each tuple contains n unigrams\n",
    "\n",
    "    :param input_list:\n",
    "    :param n:\n",
    "    :return:\n",
    "    >>> find_ngrams(['A', 'B', 'C', 'D'], 2)\n",
    "    [('A', 'B'), ('B', 'C'), ('C', 'D')]\n",
    "    >>> find_ngrams(['A', 'B', 'C', 'D'], 3)\n",
    "    [('A', 'B', 'C'), ('B', 'C', 'D')]\n",
    "    \"\"\"\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "def create_ngram_from_tokens(tokens):\n",
    "    \"\"\" list(str) -> list(str)\n",
    "\n",
    "    Returns a list of ngram strings from a list of ngram tokens\n",
    "\n",
    "    :param tokens:\n",
    "    :return:\n",
    "    >>> create_ngram_from_tokens(['A', 'B', 'C', 'D'])\n",
    "    ['A', 'B', 'C', 'D', 'A_B', 'B_C', 'C_D', 'A_B_C', 'B_C_D']\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_list = find_ngrams(tokens, 2)\n",
    "    trigram_list = find_ngrams(tokens, 3)\n",
    "\n",
    "    bigrams = [tuple[0] + '_' + tuple[1] for tuple in bigram_list]\n",
    "    trigrams = [tuple[0] + '_' + tuple[1] + '_' + tuple[2] for tuple in trigram_list]\n",
    "\n",
    "    ngram_list = tokens + bigrams + trigrams\n",
    "    return ngram_list\n",
    "\n",
    "\n",
    "def create_ngram(df, title='title'):\n",
    "    \"\"\" (DataFrame) -> DataFrame\n",
    "\n",
    "    Returns a DataFrame where the title is converted from str to ngrams\n",
    "\n",
    "    :param df:\n",
    "    :param title:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    df[title] = df[title].apply(create_ngram_from_tokens)\n",
    "    logger.info('{} ngrams created'.format(title))\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_tfidf_dict(train, title='title', category='regional_key'):\n",
    "    \"\"\" (DataFrame, str, str) -> defaultdict\n",
    "\n",
    "    Returns a tf-idf dict given a dataframe containing title and regional_key\n",
    "\n",
    "    :param train:\n",
    "    :param title:\n",
    "    :param category:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create tf dictionary (though the name is tfidf, it's only tf for now)\n",
    "    ngram_dict_tfidf = defaultdict()\n",
    "\n",
    "    # For each token in the titles, create a dict as its value\n",
    "    for i, row in train.iterrows():\n",
    "        tokens = row[title]\n",
    "        for token in tokens:\n",
    "            ngram_dict_tfidf[token] = defaultdict()\n",
    "\n",
    "    logger.info('TF dict phase 1 done')\n",
    "\n",
    "    # For each token in the titles, add the token frequency to the value of the token key\n",
    "    # Token frequency = token count / total number of tokens in title\n",
    "    for i, row in train.iterrows():\n",
    "        tokens = row[title]\n",
    "        regional_id = row[category]\n",
    "        for token in tokens:\n",
    "            token_tf = tokens.count(token) / float(len(tokens))\n",
    "            try:\n",
    "                ngram_dict_tfidf[token][regional_id] += token_tf\n",
    "            except KeyError:\n",
    "                ngram_dict_tfidf[token][regional_id] = token_tf\n",
    "\n",
    "    logger.info('TF dict phase 2 done')\n",
    "\n",
    "    # create idf dictionary and count the number of titles in train\n",
    "    ngram_dict_idf = defaultdict()\n",
    "    no_of_skus = len(train)\n",
    "\n",
    "    # For each token in the title, add one to the value of the token key\n",
    "    for i, row in train.iterrows():\n",
    "        tokens = set(row[title])\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                ngram_dict_idf[token] += 1\n",
    "            except KeyError:\n",
    "                ngram_dict_idf[token] = 1\n",
    "\n",
    "    # For each token in idf dict, divide the total number of skus (logged) by the count of token value\n",
    "    # Add 1 to the numerator to prevent zero divison error\n",
    "    for term, count in ngram_dict_idf.iteritems():\n",
    "        ngram_dict_idf[term] = math.log(no_of_skus) / float(1 + count)\n",
    "\n",
    "    logger.info('IDF dict done')\n",
    "\n",
    "    # Multiple values in tf dictionary with idf dictionary to get tf-idf dictionary\n",
    "    for ngram, cat_dict in ngram_dict_tfidf.iteritems():\n",
    "        # print ngram\n",
    "        ngram_idf = ngram_dict_idf[ngram]\n",
    "        # print ngram_idf\n",
    "        for regional_key, count in cat_dict.iteritems():\n",
    "            ngram_dict_tfidf[ngram][regional_key] = count * ngram_idf\n",
    "\n",
    "    logger.info('TF-IDF dict done')\n",
    "    return ngram_dict_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dicts, defaultdict=defaultdict, int=int):\n",
    "    \"\"\" (list(dict), type, type) -> dict\n",
    "\n",
    "    Returns a single dictionary given a list of dictionaries.\n",
    "    Values with the same keys are summed and assigned to the key.\n",
    "\n",
    "    :param dicts:\n",
    "    :param defaultdict:\n",
    "    :param int:\n",
    "    :return:\n",
    "\n",
    "    >>> merge_dicts([{'A': 1}, {'B': 2}])\n",
    "    defaultdict(<type 'int'>, {'A': 1, 'B': 2})\n",
    "    >>> merge_dicts([{'A': 1}, {'B': 2}, {'C': 3}, {'A': 10}])\n",
    "    defaultdict(<type 'int'>, {'A': 11, 'C': 3, 'B': 2})\n",
    "    \"\"\"\n",
    "\n",
    "    merged = defaultdict(int)\n",
    "    for d in dicts:\n",
    "        for k in d:\n",
    "            merged[k] += d[k]\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def get_score(tokens, ngram_dict, top_n):\n",
    "    dict_list = []\n",
    "\n",
    "    # get list of dictionaries based on tokens\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            dict_list.append(ngram_dict[token])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # Merge list of dicts together and add values\n",
    "    score = merge_dicts(dict_list)\n",
    "\n",
    "    # Get top n regional ids based on score\n",
    "    top_n_cats = heapq.nlargest(top_n, score, key=score.get)\n",
    "\n",
    "    return top_n_cats\n",
    "\n",
    "\n",
    "def get_top_n_score(scores, n):\n",
    "    try:\n",
    "        n_score = scores[n - 1]\n",
    "    except IndexError:\n",
    "        n_score = -1\n",
    "\n",
    "    return n_score\n",
    "\n",
    "\n",
    "def create_options(df, title, tfidf_dict):\n",
    "    df['options'] = df.loc[:, title].apply(get_score, args=(tfidf_dict, 3, ))\n",
    "    logger.info('Test set scored')\n",
    "    \n",
    "    df['option1'] = df.loc[:, 'options'].apply(get_top_n_score, args=(1, ))\n",
    "    df['option2'] = df.loc[:, 'options'].apply(get_top_n_score, args=(2, ))\n",
    "    df['option3'] = df.loc[:, 'options'].apply(get_top_n_score, args=(3, ))\n",
    "    logger.info('Top 3 options created')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_accuracy(df):\n",
    "    \n",
    "    df['option1_match'] = df['category_path'] == df['option1']\n",
    "    df['option2_match'] = df['category_path'] == df['option2']\n",
    "    df['option3_match'] = df['category_path'] == df['option3']\n",
    "    \n",
    "    score1 = df['option1_match'].sum() / float(len(df))\n",
    "    score2 = df['option2_match'].sum() / float(len(df))\n",
    "    score3 = df['option3_match'].sum() / float(len(df))\n",
    "    score123 = score1 + score2 + score3\n",
    "    \n",
    "    print \"Scores: {}, {}, {} ({})\".format(score1, score2, score3, score123)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-12 12:15:00,509 - title encoded\n",
      "INFO:__log__:title encoded\n",
      "2016-06-12 12:15:02,082 - title lowercased\n",
      "INFO:__log__:title lowercased\n",
      "2016-06-12 12:15:10,503 - title tokenized\n",
      "INFO:__log__:title tokenized\n",
      "2016-06-12 12:15:13,853 - title stopwords removed\n",
      "INFO:__log__:title stopwords removed\n",
      "2016-06-12 12:15:16,143 - title solely numeric words removed\n",
      "INFO:__log__:title solely numeric words removed\n",
      "2016-06-12 12:15:18,992 - title tokens with char length equals 1 removed\n",
      "INFO:__log__:title tokens with char length equals 1 removed\n",
      "2016-06-12 12:15:30,655 - title ngrams created\n",
      "INFO:__log__:title ngrams created\n",
      "2016-06-12 12:15:30,656 - Data prepped\n",
      "INFO:__log__:Data prepped\n"
     ]
    }
   ],
   "source": [
    "# # Process titles\n",
    "df = encode_title(df, title='title') \n",
    "df = lowercase_title(df, title='title')\n",
    "df = tokenize_title(df, title='title', excluded='-.')\n",
    "df = remove_stopwords(df, stopwords=STOP_WORDS, title='title')\n",
    "df = remove_numeric_from_df(df, title='title')\n",
    "df = remove_one_char_words(df, word_len=1, title='title')\n",
    "df = create_ngram(df, title='title')\n",
    "logger.info('Data prepped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-12 12:20:57,635 - Train test split done: Train((891216, 3)), Test((99862, 3))\n",
      "INFO:__log__:Train test split done: Train((891216, 3)), Test((99862, 3))\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, stratify=df['category_path'], random_state=1368)\n",
    "logger.info('Train test split done: Train({}), Test({})'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-12 12:23:01,456 - TF dict phase 1 done\n",
      "INFO:__log__:TF dict phase 1 done\n",
      "2016-06-12 12:24:45,645 - TF dict phase 2 done\n",
      "INFO:__log__:TF dict phase 2 done\n",
      "2016-06-12 12:26:05,244 - IDF dict done\n",
      "INFO:__log__:IDF dict done\n",
      "2016-06-12 12:26:13,736 - TF-IDF dict done\n",
      "INFO:__log__:TF-IDF dict done\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict = create_tfidf_dict(train=train, title='title', category='category_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "2016-06-12 12:30:39,083 - Test set scored\n",
      "INFO:__log__:Test set scored\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "2016-06-12 12:30:58,499 - Top 3 options created\n",
      "INFO:__log__:Top 3 options created\n"
     ]
    }
   ],
   "source": [
    "test = create_options(test, 'title', tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.606226592698, 0.110662714546, 0.0472051430975 (0.764094450341)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test = validate_accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation results (1 million samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer: -.; remove numerics\n",
    "# Scores: 0.605795998478, 0.110532534898, 0.0475456129459 (0.763874146322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer: none; remove numerics\n",
    "# Scores: 0.605505597725, 0.111393723338, 0.0476257234984 (0.764525044561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer: none; remove numerics, no trigrams\n",
    "# Scores: 0.585818429433, 0.120696561255, 0.0509102561535 (0.757425246841)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer: -.; remove numerics; remove html\n",
    "# Scores: 0.606226592698, 0.110662714546, 0.0472051430975 (0.764094450341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation results (full data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer: none; remove numerics\n",
    "# Scores: 0.680834112035, 0.109327693897, 0.0413770717553 (0.831538877687)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'match'] = test.loc[:, 'option1_match'] + test.loc[:, 'option2_match'] + test.loc[:, 'option3_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check = test[test['match'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['asin', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check = check.merge(df, how='left', on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = check[['title_y', 'title_x', 'category_path', 'option1', 'option2', 'option3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
